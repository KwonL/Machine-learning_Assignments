{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cREYXQHuKNiK"
   },
   "source": [
    "# Language Modeling with CharRNN\n",
    "\n",
    "Adapted by Sungwon Kim from the work of [Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow/) and [SOLARIS](https://github.com/solaris33/char-rnn-tensorflow)\n",
    "\n",
    "## Char-RNN\n",
    "\n",
    "This code implements multi-layer Recurrent Neural Network (RNN, LSTM, and GRU) for training/sampling from character-level language models. In other words the model takes one text file as input and trains a Recurrent Neural Network that learns to predict the next character in a sequence. The RNN can then be used to generate text character by character that will look like the original training data. The context of this code base is described in detail in [Karpahty's blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). *Explanation from Andrej's code*\n",
    "\n",
    "Original code (written in Torch/Lua) : https://github.com/karpathy/char-rnn\n",
    "\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" alt=\"nn\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "$ \\log{p(x_{1:T})} = \\sum_{t} log{p(x_t|x_<{t})} $\n",
    "\n",
    "\n",
    "\n",
    "## Character-level language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GeIO21tMKzOd"
   },
   "source": [
    "### Mount Google Drive (ONLY for students using Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPBTI1BlK17S"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVtsLcWVLRMM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9U9Fbw5qLTkD"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZddB11k_KNiT"
   },
   "outputs": [],
   "source": [
    "# load a bunch of libraries\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# this module is from the utils.py file of this folder\n",
    "# it handles loading texts to digits (aka. tokens) which are recognizable for the model\n",
    "from utils import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "101-5ZGPXVbK"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhI2ULFbKNiX"
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/tinyshakespeare'\n",
    "batch_size = 50\n",
    "seq_length = 50\n",
    "num_hidden = 256\n",
    "learning_rate = 0.002\n",
    "num_epochs = 2\n",
    "num_layers = 2\n",
    "grad_clip = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njPQIzQYXlad"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFs6cTdqKNia"
   },
   "outputs": [],
   "source": [
    "# protip: always check the data and poke around the data yourself\n",
    "# you will get a lot of insights by looking at the data\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "chars = data_loader.chars\n",
    "vocab = data_loader.vocab\n",
    "vocab_size = data_loader.vocab_size\n",
    "print(vocab_size)\n",
    "\n",
    "data_loader.reset_batch_pointer()\n",
    "\n",
    "x, y = data_loader.next_batch()\n",
    "\n",
    "# our data has a shape of (N, T), where N=batch_size and T=seq_length\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BggqRV2MKNid",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see what the first entry of the batch look like\n",
    "print(x[0])\n",
    "# y is just an x shifted to the left by one: so the network will predict the next token y given x. \n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ati-mWhHRaL4"
   },
   "source": [
    "## TF Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZUd9-SSXuFH"
   },
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-R_3QdeGSA9e"
   },
   "outputs": [],
   "source": [
    "# X : [batch_size, seq_length]\n",
    "# Y : [batch_size * seq_length, vocab_size]\n",
    "# state_batch_size : batch_size (Training) / 1 (Sampling)\n",
    "X = tf.placeholder(tf.int32, [None, None])\n",
    "Y = tf.placeholder(tf.int32, [None, None])\n",
    "state_batch_size = tf.placeholder(tf.int32, shape=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VIoLAK4dYcW2"
   },
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXGYYDRsUqxx"
   },
   "outputs": [],
   "source": [
    "# embedding_lookup : [batch_size, seq_length] --> [batch_size, seq_length, hidden_size]\n",
    "\n",
    "embedding = tf.Variable(tf.random_normal(shape=[vocab_size, num_hidden]), dtype=tf.float32)\n",
    "inputs = tf.nn.embedding_lookup(embedding, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riGSzNTjYneM"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Icb9N82TYn-W"
   },
   "outputs": [],
   "source": [
    "assert num_layers > 0\n",
    "if num_layers == 1:\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.LSTMBlockCell(num_hidden, forget_bias=1.0)\n",
    "else:\n",
    "    cells = [rnn.LSTMBlockCell(num_hidden, forget_bias=1.0) for _ in range (num_layers)]\n",
    "    lstm_cell = rnn.MultiRNNCell(cells)\n",
    "    \n",
    "initial_state = lstm_cell.zero_state(state_batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rh2TIfHFRV0V"
   },
   "outputs": [],
   "source": [
    "# Get lstm cell output\n",
    "# time_major=True --> inputs_shape : [timesteps, batch_size, num_hidden]\n",
    "# time_major=False --> inputs_shape : [batch_size, timesteps, num_hidden]\n",
    "# outputs : [batch_size, timesteps, num_hidden]\n",
    "outputs, final_states = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=inputs, initial_state=initial_state,\n",
    "                                    time_major=False, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TdtbGyXlccaN"
   },
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXuyZm5bbqRT"
   },
   "outputs": [],
   "source": [
    "# output : [batch_size, timesteps, num_hidden] --> [batch_size * timesteps, num_hidden]\n",
    "# logits : [batch_size * timesteps, vocab_size]\n",
    "outputs = tf.reshape(outputs, [-1, num_hidden])\n",
    "logits = tf.layers.dense(outputs, vocab_size)\n",
    "probs = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZr4Nm_Md3Ti"
   },
   "source": [
    "### Define Loss, Optimizer, and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TmDjYIq7d6vE"
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('optimizer'):\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y))\n",
    "    # Gradient clipping to avoid exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss_op, tvars), 1.)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6eH--f6RVjH"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fa5tTju0RXAV"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "sess = tf.Session()\n",
    "    \n",
    "# Run the initializer\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1, num_epochs+1):\n",
    "    data_loader.reset_batch_pointer()\n",
    "\n",
    "    state = sess.run(initial_state, feed_dict={state_batch_size : batch_size})\n",
    "\n",
    "    for b in range(data_loader.num_batches):\n",
    "        # batch_x : [batch_size, seq_length]\n",
    "        batch_x, batch_y = data_loader.next_batch()\n",
    "\n",
    "        # batch_y : [batch_size, seq_length] --> [batch_size, seq_length, vocab_size]\n",
    "        batch_y = tf.one_hot(batch_y, vocab_size)            \n",
    "        # batch_y : [batch_size, seq_length, vocab_size] --> [batch_size * seq_length, vocab_size]\n",
    "        batch_y = tf.reshape(batch_y, [-1, vocab_size])\n",
    "        batch_y = batch_y.eval(session=sess)\n",
    "\n",
    "        feed_dict = {X : batch_x, Y: batch_y, state_batch_size : batch_size}\n",
    "        for i, (c, h) in enumerate(initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "        _, loss_print, state = sess.run([train_op, loss_op, final_states], feed_dict=feed_dict)\n",
    "\n",
    "        if ((step-1) * data_loader.num_batches + b) % 50 == 0:\n",
    "            print(\"{}/{}, Epoch: {}, Loss: {:.3f}\".format(\n",
    "                          (step-1) * data_loader.num_batches + b,\n",
    "                          num_epochs * data_loader.num_batches,\n",
    "                          step, \n",
    "                          loss_print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBJc3qNth7vr"
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TOhcQ95h7AT"
   },
   "outputs": [],
   "source": [
    "print(\"Sampling\")\n",
    "num_sampling = 4000\n",
    "sampling_batch_size = 1\n",
    "prime = u' '         # start token : ' '\n",
    "sampling_type = 2   \n",
    "state = sess.run(lstm_cell.zero_state(sampling_batch_size, tf.float32)) # zero states for initializing RNN statea\n",
    "\n",
    "# function for random sampling\n",
    "def weighted_pick(weights):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "ret = prime       # sampling result\n",
    "char = prime[-1]  # current input\n",
    "for n in range(num_sampling):\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0, 0] = vocab[char]\n",
    "\n",
    "    # One RNN step computation\n",
    "    feed_dict = {X: x, state_batch_size : 1, initial_state: state}\n",
    "    [probs_result, state] = sess.run([probs, final_states], feed_dict=feed_dict)         \n",
    "\n",
    "    # probs_result : (1,65) -> p : (65)\n",
    "    p = np.squeeze(probs_result)\n",
    "\n",
    "    # Sampling Type\n",
    "    # 0 : argmax sampling\n",
    "    # 1 : random sampling\n",
    "    # 2 : argmax sampling (previous token != ' '), random sampling (previous token == ' ')\n",
    "    if sampling_type == 0:\n",
    "        sample = np.argmax(p)\n",
    "    elif sampling_type == 2:\n",
    "        if char == ' ':\n",
    "            sample = weighted_pick(p)\n",
    "        else:\n",
    "            sample = np.argmax(p)\n",
    "    else:\n",
    "        sample = weighted_pick(p)\n",
    "\n",
    "    pred = chars[sample]\n",
    "    ret += pred     # Update sampling result\n",
    "    char = pred     # Update current result\n",
    "\n",
    "print(\"Sampling result : {}\".format(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dTnsR8qrtLV"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CharRNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
